{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to Dataframe Using Multiprocessing Pool\n",
    "\n",
    "#### Background Context\n",
    "**Multiprocessing** is a useful python package that enables users to utilize multiple processors on a given machine for more efficient progress. The Pool object allows the exploitation of data parallelism by distributing the work across a pool of processes running the same function. This greatly improves the speed at which the work is done, reducing overall runtime.\n",
    "\n",
    "Multiprocessing is mainly preferred when calling functions on larger sets of data expressing data parallelism. Data parallelism is the concept of breaking a set of data into smaller sets, which is then processed on multiple processes applying the same function without communicating with each other. Joining the output of these processes should produce the same result as if one process had applied the function to the entire dataset.\n",
    "\n",
    "#### Script Purpose\n",
    "This script is the multiprocessing variation of the **Convert to Dataframe** script. In this sample script, we will demonstrate the use of Multiprocessing Pool in parsing large numbers of XML files. We will create the function for parsing, create a pool object, and then call the function using that pool object to run via multiprocessing.\n",
    "\n",
    "#### Expected Runtime and Sample Size\n",
    "Mileage will vary depending on document size and resources available. On the standard notebook instance with 4 cores, expect around 8 minutes of runtime to process 53k New York Times articles and around 2 minutes of runtime to process 5k dissertations. The beginning input size is set to the corpus `SAMPLEDATA`, which includes around 53k articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for parsing data\n",
    "from lxml import etree\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Libraries for multiprocessing\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 15516 documents.\n"
     ]
    }
   ],
   "source": [
    "# Set corpus to the folder of files you want to use\n",
    "corpus_bernanke = '/home/ec2-user/SageMaker/data/Global_Newsstream_2013_bernanke/'\n",
    "\n",
    "# Read in files\n",
    "input_files_bernanke = os.listdir(corpus_bernanke)\n",
    "\n",
    "print(\"Loaded\", len(input_files_bernanke), \"documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Output File\n",
    "\n",
    "Define the `output_file` variable to the desired save location and file name. This variable will be used at the end of the script to save the processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify output_file to desired save name\n",
    "output_file = 'output_files/Global_Newsstream_2013_bernanke.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Total Cores\n",
    "\n",
    "Check the total number of cores on your current device. The following multiprocessing portions will be using this variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# Check core count\n",
    "num_cores = mp.cpu_count()\n",
    "print(num_cores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to strip html tags from text portion\n",
    "def strip_html_tags(text):\n",
    "    stripped = BeautifulSoup(text).get_text().replace('\\n', ' ').replace('\\\\', '').strip()\n",
    "    return stripped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve metadata from XML document\n",
    "def getxmlcontent(corpus, file, strip_html=True):\n",
    "    try:\n",
    "        tree = etree.parse(corpus + file)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        if root.find('.//GOID') is not None:\n",
    "            goid = root.find('.//GOID').text\n",
    "        else:\n",
    "            goid = None\n",
    "\n",
    "        if root.find('.//Title') is not None:\n",
    "            title = root.find('.//Title').text\n",
    "        else:\n",
    "            title = None\n",
    "\n",
    "        if root.find('.//NumericDate') is not None:\n",
    "            date = root.find('.//NumericDate').text\n",
    "        else:\n",
    "            date = None\n",
    "            \n",
    "        if root.find('.//PublisherName') is not None:\n",
    "            publisher = root.find('.//PublisherName').text\n",
    "        else:\n",
    "            publisher = None\n",
    "\n",
    "        if root.find('.//FullText') is not None:\n",
    "            text = root.find('.//FullText').text\n",
    "\n",
    "        elif root.find('.//HiddenText') is not None:\n",
    "            text = root.find('.//HiddenText').text\n",
    "\n",
    "        elif root.find('.//Text') is not None:\n",
    "            text = root.find('.//Text').text\n",
    "\n",
    "        else:\n",
    "            text = None\n",
    "\n",
    "        # Strip html from text portion\n",
    "        if text is not None and strip_html == True:\n",
    "            text = strip_html_tags(text)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error while parsing file {file}: {e}\")\n",
    "    \n",
    "    return goid, title, date, publisher, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make lists out of parsed data--on single document scale for multiprocessing\n",
    "def make_lists(file):\n",
    "    \n",
    "    goid, title, date, publisher, text = getxmlcontent(corpus, file, strip_html=True)\n",
    "    \n",
    "    return goid, text, date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Multiprocessing to parse XML files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test function on single document\n",
    "make_lists(input_files[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# When using multiple processes, important to eventually close them to avoid memory/resource leaks\n",
    "try:\n",
    "    # Define a thread Pool to process multiple XML files simultaneously\n",
    "    # Default set to num_cores - 1, but may change number of processes depending on instance\n",
    "    p = Pool(processes=num_cores-1)\n",
    "    \n",
    "    # Apply function with Pool to corpus\n",
    "    processed_lists = p.map(make_lists, input_files)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error in processing document: {e}\")\n",
    "    \n",
    "finally:\n",
    "    p.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform processed data into a dataframe\n",
    "df = pd.DataFrame(processed_lists, columns=['GOID', 'Text', 'Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Dataframe as CSV\n",
    "\n",
    "Make sure to change the `output_file` variable (defined at the top of script) to desired output file name before running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save output to file\n",
    "df.to_csv(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sample-2022.05.25",
   "language": "python",
   "name": "sample-2022.05.25"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

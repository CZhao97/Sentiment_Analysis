{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da348fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install xlrd==1.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45c17068",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\czhao\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\czhao\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll\n",
      "C:\\Users\\czhao\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "import xlrd\n",
    "xlrd.xlsx.ensure_elementtree_imported(False, None)\n",
    "xlrd.xlsx.Element_has_iter = True\n",
    "xls = xlrd.open_workbook(r'output_folder/CC_output_20100224.xlsx', on_demand=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e408ae5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sheets_names = xls.sheet_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b105b8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_chunk(df_slice):\n",
    "        # Check if at least one row in the slice has 'number_of_words' >= 3\n",
    "        # and non-empty col\n",
    "        valid_rows = df_slice[\n",
    "            (df_slice['number_of_words'] >= 3) &\n",
    "            (df_slice[col].astype(bool))\n",
    "        ]\n",
    "        if valid_rows.empty:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "# calculate the min distance for given sequence\n",
    "def calculate_min_seq_dist(sequence):\n",
    "        dist = 0\n",
    "        sequence = list(itertools.chain.from_iterable(sequence))\n",
    "        for idx, val in enumerate(sequence[:-1]):\n",
    "            dist += abs(val - sequence[idx+1])\n",
    "        return dist\n",
    "\n",
    "def non_empty(sequence):\n",
    "    count = 0\n",
    "    for val in sequence[:-1]:\n",
    "        if val != []:\n",
    "            count +=1\n",
    "    return count\n",
    "\n",
    "def merge_to_tuples(list1, list2):\n",
    "    merged_list = [(list1[i], list2[i]) for i in range(0, len(list1))]\n",
    "    return merged_list\n",
    " \n",
    "\n",
    "#     find the start point for forward and backward\n",
    "def find_starting_point(chunk_df):\n",
    "    max_words = chunk_df[chunk_df[col].astype(bool)]['number_of_words'].max()\n",
    "    starting_point_found = False\n",
    "    correct_sent_num = None\n",
    "    correct_row = None\n",
    "    min_dist = 100000000\n",
    "\n",
    "    while not starting_point_found and max_words >= 3:\n",
    "        valid_rows = chunk_df[(chunk_df['number_of_words'] == max_words) & (chunk_df[col].astype(bool))]\n",
    "        if not valid_rows.empty and len(valid_rows.iloc[0][col]) > 1:\n",
    "            best_sequence = []\n",
    "            for sent_num in valid_rows.iloc[0][col]:\n",
    "                sequence = compute_sequence(chunk_df, sent_num, valid_rows.index.values[0])\n",
    "\n",
    "                if non_empty(sequence) == non_empty(best_sequence):\n",
    "                    if calculate_min_seq_dist(sequence) < calculate_min_seq_dist(best_sequence):\n",
    "                        best_sequence = sequence\n",
    "                        correct_sent_num = [sent_num]\n",
    "                        correct_row = [valid_rows.index.values[0]]\n",
    "                    elif calculate_min_seq_dist(sequence) == calculate_min_seq_dist(best_sequence):\n",
    "                        best_sequence = sequence\n",
    "                        correct_sent_num.append(sent_num)\n",
    "                        correct_row.append(valid_rows.index.values[0])\n",
    "\n",
    "                elif non_empty(sequence) > non_empty(best_sequence):    \n",
    "                    best_sequence = sequence\n",
    "                    correct_sent_num = [sent_num]\n",
    "                    correct_row = [valid_rows.index.values[0]]\n",
    "\n",
    "            starting_point_found = True\n",
    "        elif not valid_rows.empty:\n",
    "            correct_sent_num = [valid_rows.iloc[0][col][0]]\n",
    "            correct_row = [valid_rows.index.values[0]]\n",
    "            starting_point_found = True\n",
    "        else:\n",
    "            max_words -= 1\n",
    "            \n",
    "    return zip(*list(set(merge_to_tuples(correct_sent_num, correct_row))))\n",
    "\n",
    "    \n",
    "# get exact longest sequences\n",
    "def compute_sequence(chunk_df, initial_sent_num, initial_sent_row):\n",
    "    last_correct_sent_num = initial_sent_num\n",
    "    sequence_forward = []\n",
    "    sequence_backward = []\n",
    "\n",
    "\n",
    "    for index, row in chunk_df.loc[initial_sent_row:,:].iterrows():\n",
    "        current_values = row[col]\n",
    "        if not current_values:\n",
    "            sequence_forward.append((index, []))\n",
    "            continue\n",
    "\n",
    "        valid_values = [x for x in current_values if x >= last_correct_sent_num]\n",
    "\n",
    "        if valid_values:\n",
    "            min_distance = min(abs(x - last_correct_sent_num) for x in valid_values)\n",
    "            if min_distance <= 10:\n",
    "                new_correct = min(x for x in valid_values if abs(x - last_correct_sent_num) == min_distance)\n",
    "                last_correct_sent_num = new_correct\n",
    "\n",
    "                sequence_forward.append((index, new_correct))\n",
    "            else:\n",
    "                sequence_forward.append((index, []))\n",
    "\n",
    "    last_correct_sent_num = initial_sent_num\n",
    "\n",
    "    for index, row in chunk_df.loc[:initial_sent_row-1,:].iloc[::-1].iterrows():\n",
    "        current_values = row[col]\n",
    "        if not current_values:\n",
    "            sequence_backward.insert(0, (index, []))\n",
    "            continue\n",
    "\n",
    "        valid_values = [x for x in current_values if x <= last_correct_sent_num]\n",
    "        if valid_values:\n",
    "\n",
    "            min_distance = min(abs(x - last_correct_sent_num) for x in valid_values)\n",
    "\n",
    "            if min_distance <= 10:\n",
    "                new_correct = min(x for x in valid_values if abs(x - last_correct_sent_num) == min_distance)\n",
    "                last_correct_sent_num = new_correct\n",
    "                sequence_backward.insert(0, (index, new_correct))\n",
    "            else:\n",
    "                sequence_backward.insert(0, (index, []))\n",
    "\n",
    "    merged_dict = {i:[] for i in range(start, end)}\n",
    "\n",
    "\n",
    "    # Insert values from list1 into merged_dict\n",
    "    for index, value in sequence_forward:\n",
    "        if value != [] and value not in merged_dict[index]:\n",
    "            merged_dict[index].append(value)\n",
    "\n",
    "    # Insert values from list2 into merged_dict\n",
    "    for index, value in sequence_backward:\n",
    "        if value != [] and value not in merged_dict[index]:\n",
    "            merged_dict[index].append(value)\n",
    "\n",
    "    # Convert merged_dict back to list of (index, value) pairs\n",
    "    merged_list = [values for index, values in merged_dict.items()]\n",
    "\n",
    "    return merged_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4a30bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('output_folder/CC_output_20100224.xlsx') \n",
    "df_transcript = pd.read_excel(open('output_folder/CC_output_20100224.xlsx', 'rb'),\n",
    "              sheet_name='Transcript') \n",
    "\n",
    "proc_cols = ['occurrence_sent_num', 'timed_matches']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "073c6435",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in proc_cols:\n",
    "    df[col] = df[col].replace({np.nan: '[]'})\n",
    "    df[col] = df.apply(lambda x: literal_eval(x[col]), axis=1)\n",
    "\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(df):\n",
    "        count_empty = 0\n",
    "        j = i\n",
    "        while j < len(df) and (count_empty <= 2):\n",
    "            if not df.iloc[j][col]:\n",
    "                count_empty += 1\n",
    "            else:\n",
    "                count_empty = 0\n",
    "            j += 1\n",
    "        # If the chunk has at least 5 rows and is valid\n",
    "        if (j - i) >= 5 and is_valid_chunk(df.iloc[i:j]):\n",
    "            chunks.append((i, j))\n",
    "            i = j\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "    for start, end in chunks:\n",
    "\n",
    "        chunk_df = df.iloc[start:end].copy()\n",
    "\n",
    "        correct_sent_num, correct_row = find_starting_point(chunk_df)\n",
    "\n",
    "        longest_sequence_series_ls = []\n",
    "\n",
    "        for (i, j) in zip(correct_sent_num, correct_row):\n",
    "            # Compute the sequence with the longest valid set of timed_matches values\n",
    "            longest_sequence = compute_sequence(chunk_df, i, j)\n",
    "\n",
    "            occurred_flag = 0\n",
    "            modified_longest_sequence = []\n",
    "            for idx, val in enumerate(longest_sequence):\n",
    "                if val!= []:\n",
    "                    occurred_flag = 1\n",
    "                    if idx > 0 and idx < len(longest_sequence)-1:\n",
    "                        if longest_sequence[idx-1]==[] and longest_sequence[idx+1]==[]:\n",
    "                            modified_longest_sequence.append(chunk_df[col].iloc[idx])\n",
    "                        else:\n",
    "                            modified_longest_sequence.append(val)\n",
    "                    elif idx == 0:\n",
    "                        if longest_sequence[1] == []:\n",
    "                            modified_longest_sequence.append(chunk_df[col].iloc[idx])\n",
    "                        else:\n",
    "                            modified_longest_sequence.append(val)\n",
    "                    elif idx == len(longest_sequence)-1:\n",
    "                        if longest_sequence[len(longest_sequence)-2] == []:\n",
    "                            modified_longest_sequence.append(chunk_df[col].iloc[idx])\n",
    "                        else:\n",
    "                            modified_longest_sequence.append(val)                   \n",
    "\n",
    "                else: \n",
    "                    occurred_flag = 0\n",
    "                    modified_longest_sequence.append(val)\n",
    "\n",
    "            longest_sequence_series = modified_longest_sequence\n",
    "            if not bool(longest_sequence_series_ls):\n",
    "                longest_sequence_series_ls = longest_sequence_series\n",
    "            else:\n",
    "                longest_sequence_series_ls = [sublist1 + sublist2 for sublist1, sublist2 in zip(longest_sequence_series_ls, longest_sequence_series)]\n",
    "\n",
    "\n",
    "        df.loc[start:end-1, 'Longest_Sequence_' + col] = pd.Series(longest_sequence_series_ls, index=df.iloc[start:end].index)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74b49918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frag_in_elapse_time(df, based_col, new_col, gap_time=35):\n",
    "    # Assuming you have a DataFrame named 'df' with appropriate columns\n",
    "\n",
    "    last_sent = 0\n",
    "    next_sent = 0\n",
    "    elapse_time0 = 0\n",
    "#     gap_time = 60\n",
    "    \n",
    "    df[new_col] = pd.Series(dtype='int')\n",
    "\n",
    "    # Iterate over each row in the DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        frag_count_gt1 = row[based_col]\n",
    "        time_span = row['time_span']\n",
    "\n",
    "        next_sent = index\n",
    "        if frag_count_gt1 > 0:\n",
    "            # Set indicator = 1 and update last_sent\n",
    "            df.at[index, new_col] = 1\n",
    "            last_sent = index\n",
    "            elapse_time0 = 0\n",
    "        else:\n",
    "            if last_sent == 0:\n",
    "                # Set indicator = 0 if no matches found yet\n",
    "                df.at[index, new_col] = 0\n",
    "            else:\n",
    "\n",
    "                elapse_time0 += time_span\n",
    "\n",
    "                if elapse_time0 <= gap_time:\n",
    "                    # Set indicator = 1 if time span <= gap_time\n",
    "                    df.at[index, new_col] = 1\n",
    "                else:\n",
    "                    # Set indicator = 0 for rows between last_sent+1 and next_sent-1\n",
    "                    df.loc[last_sent + 1:next_sent, new_col] = 0\n",
    "                    elapse_time0 = 0\n",
    "                    last_sent=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c04e20cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\czhao\\AppData\\Local\\Temp\\ipykernel_2720\\3695093394.py:5: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  occurrence_dic = Counter(list(itertools.chain.from_iterable(df[df['cleaned_text'].str.len()>1][df[col].apply(lambda x: len(x)==1)][col].values)))\n",
      "C:\\Users\\czhao\\AppData\\Local\\Temp\\ipykernel_2720\\3695093394.py:5: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  occurrence_dic = Counter(list(itertools.chain.from_iterable(df[df['cleaned_text'].str.len()>1][df[col].apply(lambda x: len(x)==1)][col].values)))\n"
     ]
    }
   ],
   "source": [
    "# recalculate frag for Longest_Sequence_occurrence_sent_num and Longest_Sequence_timed_matches\n",
    "\n",
    "for col in ['Longest_Sequence_occurrence_sent_num', 'Longest_Sequence_timed_matches']:\n",
    "\n",
    "    df.loc[df[col].isnull(),[col]] = df.loc[df[col].isnull(),col].apply(lambda x: [])\n",
    "\n",
    "    occurrence_dic = Counter(list(itertools.chain.from_iterable(df[df['cleaned_text'].str.len()>1][df[col].apply(lambda x: len(x)==1)][col].values)))\n",
    "\n",
    "    df_transcript['Frag_count_gt'+'_'+col] = df_transcript['sentence_num'].apply(lambda x: occurrence_dic[x] if x in occurrence_dic else 0)\n",
    "\n",
    "    get_frag_in_elapse_time(df_transcript, 'Frag_count_gt'+'_'+col, 'Frag_in_elasp_time'+'_'+col)\n",
    "\n",
    "    # Add two dummy columns\n",
    "    df_transcript['dummy_preceding'+'_'+col] = 0\n",
    "    df_transcript['dummy_subsequent'+'_'+col] = 0\n",
    "\n",
    "    # Iterate over each row in the DataFrame\n",
    "    for index, row in df_transcript.iterrows():\n",
    "        # Calculate the sum of the preceding 5 sentences\n",
    "        if index > 4:\n",
    "            preceding_sum = df_transcript['Frag_in_elasp_time'+'_'+col][index - 5:index].sum()\n",
    "        else:\n",
    "            preceding_sum = df_transcript['Frag_in_elasp_time'+'_'+col][0:index].sum()\n",
    "\n",
    "        # Calculate the sum of the subsequent 5 sentences or till the end of the file\n",
    "        if index < len(df) - 5:\n",
    "            subsequent_sum = df_transcript['Frag_in_elasp_time'+'_'+col][index + 1:index + 6].sum()\n",
    "        else:\n",
    "            subsequent_sum = df_transcript['Frag_in_elasp_time'+'_'+col][index + 1:].sum()\n",
    "\n",
    "        # Set dummy variables\n",
    "        df_transcript.at[index, 'dummy_preceding'+'_'+col] = 1 if preceding_sum > 0 else 0\n",
    "        df_transcript.at[index, 'dummy_subsequent'+'_'+col] = 1 if subsequent_sum > 0 else 0\n",
    "\n",
    "    # Set the values in the third column based on the conditions\n",
    "    df_transcript['Frag_range5'+'_'+col] = 0\n",
    "\n",
    "    for index, row in df_transcript.iterrows():\n",
    "        second_value = row['Frag_in_elasp_time_'+col]\n",
    "        dummy_preceding = row['dummy_preceding'+'_'+col]\n",
    "        dummy_subsequent = row['dummy_subsequent'+'_'+col]\n",
    "\n",
    "        if second_value == 0:\n",
    "            df_transcript.at[index, 'Frag_range5'+'_'+col] = 0\n",
    "        elif second_value == 1 and dummy_preceding == 0 and dummy_subsequent == 0:\n",
    "            df_transcript.at[index, 'Frag_range5'+'_'+col] = 0\n",
    "        else:\n",
    "            df_transcript.at[index, 'Frag_range5'+'_'+col] = 1\n",
    "\n",
    "    # Drop the dummy columns if they are no longer needed\n",
    "    df_transcript = df_transcript.drop(['dummy_preceding'+'_'+col, 'dummy_subsequent'+'_'+col], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "81259ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(\"output_folder/CC_output_20100224.xlsx\") as writer:\n",
    "\n",
    "    df.to_excel(writer, sheet_name=sheets_names[0], index=False)\n",
    "    df_transcript.to_excel(writer, sheet_name=sheets_names[1], index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4147cd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be436ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b841f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b02de27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cac13cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Recursive function to get all sequences with their gaps, not used because computation limitations\n",
    "# def get_all_sequences(numbers, index=0, prev_value=None):\n",
    "#     if index == len(numbers):\n",
    "#         return [([], 0)]\n",
    "    \n",
    "#     # Skip the row if the list is empty\n",
    "#     if not numbers[index]:\n",
    "#         return [( [None] + seq, gap) for seq, gap in get_all_sequences(numbers, index + 1, prev_value)]\n",
    "    \n",
    "#     sequences = []\n",
    "    \n",
    "#     for value in numbers[index]:\n",
    "#         if prev_value is not None:\n",
    "#             current_gap = abs(value - prev_value)\n",
    "#         else:\n",
    "#             current_gap = 0\n",
    "        \n",
    "#         for subsequent_seq, subsequent_gap in get_all_sequences(numbers, index + 1, value):\n",
    "#             sequences.append(([value] + subsequent_seq, current_gap + subsequent_gap))\n",
    "    \n",
    "#     return sequences\n",
    "\n",
    "# # Get all sequences sorted by their total gap\n",
    "# sorted_sequences = sorted(get_all_sequences(df['timed_matches'][:5].tolist()), key=lambda x: x[1])\n",
    "\n",
    "# # Getting all sequences with the minimum gap (this will be the first sequence in the sorted list)\n",
    "# min_gap = sorted_sequences[0][1]\n",
    "# optimal_sequences = [seq for seq, gap in sorted_sequences if gap == min_gap]\n",
    "\n",
    "# # Printing the sequences\n",
    "# for seq in optimal_sequences:\n",
    "#     print(seq)\n",
    "#     print(\"----\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975621b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

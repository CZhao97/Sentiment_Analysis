{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6d54bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/.conda/envs/pytorch/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "import nltk\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import xml.etree.cElementTree as et\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "import nltk.data\n",
    "from os import listdir\n",
    "import pysent3 as ps\n",
    "from os.path import isfile, join\n",
    "import nltk.data\n",
    "import eng_spacysentiment\n",
    "nlp_spacy = eng_spacysentiment.load()\n",
    "\n",
    "# nltk_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "  def __init__(self, text, targets, tokenizer, max_len):\n",
    "    self.text = text\n",
    "    self.targets = targets\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "  def __len__(self):\n",
    "    return len(self.text)\n",
    "  def __getitem__(self, item):\n",
    "    text = str(self.text[item])\n",
    "    target = self.targets[item]\n",
    "    encoding = self.tokenizer.encode_plus(\n",
    "      text,\n",
    "      add_special_tokens=True,\n",
    "      max_length=self.max_len,\n",
    "      return_token_type_ids=False,\n",
    "      padding = 'max_length', #pad_to_max_length=True,\n",
    "      return_attention_mask=True,\n",
    "      return_tensors='pt',\n",
    "      truncation=True\n",
    "    )\n",
    "    return {\n",
    "      'text_text': text,\n",
    "      'input_ids': encoding['input_ids'].flatten(),\n",
    "      'attention_mask': encoding['attention_mask'].flatten(),\n",
    "      'targets': torch.tensor(target, dtype=torch.long)\n",
    "    }\n",
    "\n",
    "class SentimentClassifier(nn.Module):\n",
    "  def __init__(self, n_classes):\n",
    "    super(SentimentClassifier, self).__init__()\n",
    "    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "    self.drop = nn.Dropout(p=0.3)\n",
    "    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "  def forward(self, input_ids, attention_mask):\n",
    "    _, pooled_output = self.bert(\n",
    "      input_ids=input_ids,\n",
    "      attention_mask=attention_mask, \n",
    "      return_dict=False\n",
    "    )\n",
    "    output = self.drop(pooled_output)\n",
    "    return self.out(output)\n",
    "\n",
    "\n",
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "  ds = TextDataset(\n",
    "    text=df['Text'].to_numpy(),\n",
    "    targets=df.sentiment_value.to_numpy(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=max_len\n",
    "  )\n",
    "  return DataLoader(\n",
    "    ds,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=4\n",
    "  )\n",
    "\n",
    "\n",
    "def get_predictions(model, data_loader):\n",
    "  model = model.eval()\n",
    "  review_texts = []\n",
    "  predictions = []\n",
    "  prediction_probs = []\n",
    "  real_values = []\n",
    "  with torch.no_grad():\n",
    "    for d in data_loader:\n",
    "      texts = d[\"text_text\"]\n",
    "      input_ids = d[\"input_ids\"].to(device)\n",
    "      attention_mask = d[\"attention_mask\"].to(device)\n",
    "      targets = d[\"targets\"].to(device)\n",
    "      outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask\n",
    "      )\n",
    "      _, preds = torch.max(outputs, dim=1)\n",
    "      review_texts.extend(texts)\n",
    "      predictions.extend(preds)\n",
    "      prediction_probs.extend(outputs)\n",
    "      real_values.extend(targets)\n",
    "  predictions = torch.stack(predictions).cpu()\n",
    "  prediction_probs = torch.stack(prediction_probs).cpu()\n",
    "  real_values = torch.stack(real_values).cpu()\n",
    "  return review_texts, predictions, prediction_probs, real_values\n",
    "\n",
    "RANDOM_SEED = 0 \n",
    "PRE_TRAINED_MODEL_NAME = 'bert-large-uncased'\n",
    "FINBERT_MODEL_NAME = 'finbert'\n",
    "MAX_LEN = 85 \n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 8\n",
    "LEARNING_RATE = 1e-5\n",
    "TEST_DATA_PERCENT = 0.30\n",
    "\n",
    "def get_finbert_sentiment(sentence):\n",
    "    ###\n",
    "    # labels = {0:'neutral', 1:'positive',2:'negative'}\n",
    "    ###\n",
    "\n",
    "    inputs = tokenizer_finbert(sentence, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    outputs = finbert_model(**inputs)[0]\n",
    "#     return np.argmax(outputs.detach().numpy())\n",
    "    return nn.Softmax(dim=1)(outputs).detach().numpy()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bb198823",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/ec2-user/SageMaker/data/Party2016Oct/'\n",
    "files = [f for f in listdir(path) if isfile(join(path, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "945128ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to strip html tags from text portion\n",
    "def strip_html_tags(text):\n",
    "    stripped = BeautifulSoup(text).get_text().replace('\\n', ' ').replace('\\\\', '').strip()\n",
    "    return stripped\n",
    "\n",
    "def extract_meta(path, files):\n",
    "\n",
    "    meta = {'GOID': [],\n",
    "            'PubFrosting_Title': [],\n",
    "            'PubFrosting_SortTitle': [],\n",
    "            'CurrentTitle_Title': [],\n",
    "            'CurrentTitle_SortTitle': [],\n",
    "#             'Title': [], \n",
    "            'MpubId': [],\n",
    "            'Numeric Date': [],\n",
    "            'Title of Newspaper': [],\n",
    "            'Country': [],\n",
    "            'Text': []\n",
    "            }\n",
    "    \n",
    "    for file in files:\n",
    "        file_path = path+file\n",
    "        tree=et.parse(file_path)\n",
    "        root=tree.getroot()\n",
    "\n",
    "        # iteratre over the trees to extract metadata\n",
    "        for item in root.iter('GOID'):\n",
    "            meta['GOID'].append(item.text)\n",
    "            \n",
    "        if root.find('DFS/PubFrosting/Title') is not None:\n",
    "            meta['PubFrosting_Title'].append(root.find('DFS/PubFrosting/Title').text)\n",
    "        else:\n",
    "            meta['PubFrosting_Title'].append('')\n",
    "\n",
    "        if root.find('DFS/PubFrosting/SortTitle') is not None:\n",
    "            meta['PubFrosting_SortTitle'].append(root.find('DFS/PubFrosting/SortTitle').text)\n",
    "        else:\n",
    "            meta['PubFrosting_SortTitle'].append('')\n",
    "\n",
    "        if root.find('DFS/PubFrosting/CurrentTitle/Title') is not None:\n",
    "            meta['CurrentTitle_Title'].append(root.find('DFS/PubFrosting/CurrentTitle/Title').text)\n",
    "        else:\n",
    "            meta['CurrentTitle_Title'].append('')\n",
    "\n",
    "        if root.find('DFS/PubFrosting/CurrentTitle/SortTitle') is not None:\n",
    "            meta['CurrentTitle_SortTitle'].append(root.find('DFS/PubFrosting/CurrentTitle/SortTitle').text)\n",
    "        else:\n",
    "            meta['CurrentTitle_SortTitle'].append('')\n",
    "            \n",
    "\n",
    "#         if root.find('.//Title') is not None:\n",
    "#             meta['Title'].append(root.find('.//Title').text)\n",
    "#         else:\n",
    "#             meta['Title'].append('')\n",
    "\n",
    "\n",
    "        for item in root.iter('NumericDate'):\n",
    "            meta['Numeric Date'].append(item.text)\n",
    "            \n",
    "        for item in root.iter('PubFrosting'):\n",
    "            for title in item.iter('Title'):\n",
    "                meta['Title of Newspaper'].append(item[2].text)\n",
    "                break\n",
    "                \n",
    "        for item in root.iter('PubFrosting'):\n",
    "            for mpuid in item.iter('MpubId'):\n",
    "                meta['MpubId'].append(mpuid.text)\n",
    "                break\n",
    "        \n",
    "        country = root.find('.//Country')\n",
    "        if country is not None:\n",
    "            meta['Country'].append(country.text)\n",
    "        else:\n",
    "            meta['Country'].append('')\n",
    "            \n",
    "        if root.find('.//FullText') is not None:\n",
    "            meta['Text'].append(strip_html_tags(root.find('.//FullText').text))\n",
    "        elif root.find('.//HiddenText') is not None:\n",
    "            meta['Text'].append(strip_html_tags(root.find('.//HiddenText').text))\n",
    "        elif root.find('.//Text') is not None:\n",
    "            meta['Text'].append(strip_html_tags(root.find('.//Text').text))\n",
    "        else:\n",
    "            meta['Text'].append('')\n",
    "\n",
    "    return meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0009f248",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = extract_meta(path, files)\n",
    "\n",
    "df = pd.DataFrame(data=meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f9f3181b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "91cf9ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_republican = ['republican', \"republican's\", 'republican', 'G.O.P.', 'GOP']\n",
    "\n",
    "matches_democrat = ['democrat', \"democrats\", \"democrat's\"]\n",
    "\n",
    "df['sent_rep'] = df.apply(lambda x: [sentence for sentence in tokenizer.tokenize(x['Text']) \\\n",
    "                                    if any(match.lower() in sentence.lower() for match in matches_republican)], axis=1)\n",
    "\n",
    "df['sent_dem'] = df.apply(lambda x: [sentence for sentence in tokenizer.tokenize(x['Text']) \\\n",
    "                                    if any(match.lower() in sentence.lower() for match in matches_democrat)], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b3323b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = '/home/ec2-user/SageMaker/Getting Started/2022.05.25/ahkz_sentiment_classifier/ahkz_sentiment_classifier/'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = BertTokenizer.from_pretrained('/home/ec2-user/SageMaker/bert-large-uncased/')\n",
    "sent_model = torch.load(ROOT_PATH + 'model/3_class_fed_testimony_bert_large_uncased_best_model.pt', map_location = device)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c3f5c9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_finbert = BertTokenizer.from_pretrained('/home/ec2-user/SageMaker/finbert/')\n",
    "finbert_model = BertForSequenceClassification.from_pretrained('/home/ec2-user/SageMaker/finbert/', num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f4cb1927",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['GOID', 'PubFrosting_Title', 'PubFrosting_SortTitle',\n",
    "       'CurrentTitle_Title', 'CurrentTitle_SortTitle', 'MpubId',\n",
    "       'Numeric Date', 'Title of Newspaper', 'Country', 'Text', 'sent_rep',\n",
    "       'sent_dem']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9f8a5993",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rep = df[['GOID', 'PubFrosting_Title', 'PubFrosting_SortTitle',\n",
    "       'CurrentTitle_Title', 'CurrentTitle_SortTitle', 'MpubId',\n",
    "       'Numeric Date', 'Title of Newspaper', 'Country', 'sent_rep']].explode(['sent_rep'])\\\n",
    "       .reset_index(drop=True).rename(columns={\"sent_rep\": \"Text\"})\\\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c655d65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dem = df[['GOID', 'PubFrosting_Title', 'PubFrosting_SortTitle',\n",
    "       'CurrentTitle_Title', 'CurrentTitle_SortTitle', 'MpubId',\n",
    "       'Numeric Date', 'Title of Newspaper', 'Country', 'sent_dem']].explode(['sent_dem'])\\\n",
    "       .reset_index(drop=True).rename(columns={\"sent_dem\": \"Text\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8a0316b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_file(df):\n",
    "    \n",
    "    df.dropna(subset=['Text'], inplace=True)\n",
    "    \n",
    "    #finbert\n",
    "    df['finbert_score'] = df.Text.apply(get_finbert_sentiment)\n",
    "\n",
    "    df[['finbert_neutral','finbert_positive','finbert_negative']] = pd.DataFrame(df['finbert_score'].tolist(), index= df.index)\n",
    "\n",
    "    df = df.drop(['finbert_score'], axis=1)\n",
    "\n",
    "    #ahkz\n",
    "    df['sentiment_value'] = 0\n",
    "\n",
    "    testimony_data_loader = create_data_loader(df, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "    testimony_texts, predictions, prediction_probs, real_values = get_predictions(sent_model, testimony_data_loader)\n",
    "\n",
    "    pred_numpy = predictions.numpy()\n",
    "    pred_probs = prediction_probs.numpy()\n",
    "    ms_pred = real_values.numpy()\n",
    "\n",
    "    df['sentiment_head'] = pred_numpy\n",
    "    df['sentiment_head'].replace({2: -1}, inplace=True)\n",
    "    df.drop('sentiment_value', axis=1, inplace=True)\n",
    "    df = df.rename(columns={\"sentiment_head\": \"ahkz_score\"})\n",
    "\n",
    "    #pysent3\n",
    "    hiv4 = ps.HIV4()\n",
    "    df['pysent3'] = df['Text'].apply(lambda x: hiv4.get_score(hiv4.tokenize(x)))\n",
    "\n",
    "    pysent3_df = df['pysent3'].apply(pd.Series).rename(columns={\"Positive\": \"pysent3_Positive\", \\\n",
    "                                                   \"Negative\": \"pysent3_Negative\", \\\n",
    "                                                   \"Polarity\": \"pysent3_Polarity\", \\\n",
    "                                                   \"Subjectivity\": \"pysent3_Subjectivity\"})\n",
    "\n",
    "    df = pd.concat([df, pysent3_df], axis=1)\n",
    "\n",
    "    df = df.drop(columns=['pysent3'])\n",
    "    \n",
    "    df['spacy_senti'] = df['Text'].apply(lambda x: nlp_spacy(x).cats['positive'])\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de05d85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rep = processing_file(df_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe060eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dem = processing_file(df_dem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcab957",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dem = df_dem.merge(df[['GOID', 'Title of Newspaper']], on='GOID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5c9dc42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rep = df_rep.merge(df[['GOID', 'Title of Newspaper']], on='GOID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4e9ac136",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['GOID',\n",
    "         'Title',\n",
    "         'Numeric Date',\n",
    "         'Title of Newspaper',\n",
    "         'Text',\n",
    "         'finbert_neutral',\n",
    "         'finbert_positive',\n",
    "         'finbert_negative',\n",
    "         'ahkz_score',\n",
    "         'pysent3_Positive',\n",
    "         'pysent3_Negative',\n",
    "         'pysent3_Polarity',\n",
    "         'pysent3_Subjectivity',\n",
    "         'spacy_senti']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f424245a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dem = df_dem[cols]\n",
    "df_rep = df_rep[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddf46d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad32874",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rep.to_csv('/home/ec2-user/SageMaker/Getting Started/2022.05.25/ahkz_sentiment_classifier/ahkz_sentiment_classifier/party_result_2016/rep_sent.csv', index =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a091c797",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dem.to_csv('/home/ec2-user/SageMaker/Getting Started/2022.05.25/ahkz_sentiment_classifier/ahkz_sentiment_classifier/party_result_2016/dem_sent.csv', index =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d5e303df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f640591",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

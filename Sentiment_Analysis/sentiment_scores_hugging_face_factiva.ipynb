{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a08955f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\czhao\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\czhao\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll\n",
      "C:\\Users\\czhao\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "import nltk\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import xml.etree.cElementTree as et\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "import nltk.data\n",
    "from os import listdir\n",
    "import pysent3 as ps\n",
    "from os.path import isfile, join\n",
    "import eng_spacysentiment\n",
    "nlp_spacy = eng_spacysentiment.load()\n",
    "\n",
    "import textstat\n",
    "import pandas as pd\n",
    "import glob, os\n",
    "\n",
    "def sentence_process(text):\n",
    "    try:\n",
    "        return [textstat.flesch_reading_ease(text),\n",
    "                textstat.flesch_kincaid_grade(text),\n",
    "#                 textstat.smog_index(text),\n",
    "                textstat.automated_readability_index(text),\n",
    "                textstat.dale_chall_readability_score(text),\n",
    "                textstat.gunning_fog(text),\n",
    "                textstat.polysyllabcount(text)]\n",
    "    except:\n",
    "        print(text)\n",
    "        return ['','','','','','']\n",
    "\n",
    "# nltk_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "  def __init__(self, text, targets, tokenizer, max_len):\n",
    "    self.text = text\n",
    "    self.targets = targets\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "  def __len__(self):\n",
    "    return len(self.text)\n",
    "  def __getitem__(self, item):\n",
    "    text = str(self.text[item])\n",
    "    target = self.targets[item]\n",
    "    encoding = self.tokenizer.encode_plus(\n",
    "      text,\n",
    "      add_special_tokens=True,\n",
    "      max_length=self.max_len,\n",
    "      return_token_type_ids=False,\n",
    "      padding = 'max_length', #pad_to_max_length=True,\n",
    "      return_attention_mask=True,\n",
    "      return_tensors='pt',\n",
    "      truncation=True\n",
    "    )\n",
    "    return {\n",
    "      'text_text': text,\n",
    "      'input_ids': encoding['input_ids'].flatten(),\n",
    "      'attention_mask': encoding['attention_mask'].flatten(),\n",
    "      'targets': torch.tensor(target, dtype=torch.long)\n",
    "    }\n",
    "\n",
    "class SentimentClassifier(nn.Module):\n",
    "  def __init__(self, n_classes):\n",
    "    super(SentimentClassifier, self).__init__()\n",
    "    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "    self.drop = nn.Dropout(p=0.3)\n",
    "    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "  def forward(self, input_ids, attention_mask):\n",
    "    _, pooled_output = self.bert(\n",
    "      input_ids=input_ids,\n",
    "      attention_mask=attention_mask, \n",
    "      return_dict=False\n",
    "    )\n",
    "    output = self.drop(pooled_output)\n",
    "    return self.out(output)\n",
    "\n",
    "\n",
    "def create_data_loader(df, tokenizer, max_len, batch_size, col):\n",
    "  ds = TextDataset(\n",
    "    text=df[col].to_numpy(),\n",
    "    targets=df.sentiment_value.to_numpy(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=max_len\n",
    "  )\n",
    "  return DataLoader(\n",
    "    ds,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=4\n",
    "  )\n",
    "\n",
    "\n",
    "def get_predictions(model, data_loader):\n",
    "  model = model.eval()\n",
    "  review_texts = []\n",
    "  predictions = []\n",
    "  prediction_probs = []\n",
    "  real_values = []\n",
    "  with torch.no_grad():\n",
    "    for d in data_loader:\n",
    "      texts = d[\"text_text\"]\n",
    "      input_ids = d[\"input_ids\"].to(device)\n",
    "      attention_mask = d[\"attention_mask\"].to(device)\n",
    "      targets = d[\"targets\"].to(device)\n",
    "      outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask\n",
    "      )\n",
    "      _, preds = torch.max(outputs, dim=1)\n",
    "      review_texts.extend(texts)\n",
    "      predictions.extend(preds)\n",
    "      prediction_probs.extend(outputs)\n",
    "      real_values.extend(targets)\n",
    "  predictions = torch.stack(predictions).cpu()\n",
    "  prediction_probs = torch.stack(prediction_probs).cpu()\n",
    "  real_values = torch.stack(real_values).cpu()\n",
    "  return review_texts, predictions, prediction_probs, real_values\n",
    "\n",
    "RANDOM_SEED = 0 \n",
    "PRE_TRAINED_MODEL_NAME = 'bert-large-uncased'\n",
    "FINBERT_MODEL_NAME = 'finbert'\n",
    "MAX_LEN = 85 \n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 8\n",
    "LEARNING_RATE = 1e-5\n",
    "TEST_DATA_PERCENT = 0.30\n",
    "\n",
    "def get_finbert_sentiment(sentence):\n",
    "    ###\n",
    "    # labels = {0:'neutral', 1:'positive',2:'negative'}\n",
    "    ###\n",
    "\n",
    "    inputs = tokenizer_finbert(sentence, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    outputs = finbert_model(**inputs)[0]\n",
    "#     return np.argmax(outputs.detach().numpy())\n",
    "    return nn.Softmax(dim=1)(outputs).detach().numpy()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97b91fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os\n",
    "os.chdir(\"C:\\\\Users\\\\czhao\\\\OneDrive\\\\桌面\\\\uoft\\\\work_study\\\\Chen_workingfolder\\\\Powell_Titles\")\n",
    "csv_ls = []\n",
    "for file in glob.glob(\"factiva*.csv\"):\n",
    "    csv_ls.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84802908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['factiva_10days_news_20180227_Powell_direct_quotes.csv',\n",
       " 'factiva_10days_news_20180717_Powell_direct_quotes.csv',\n",
       " 'factiva_10days_news_20190226_Powell_direct_quotes.csv',\n",
       " 'factiva_10days_news_20190710_Powell_direct_quotes.csv']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44a05531",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = 'C:\\\\Users\\\\czhao\\\\OneDrive\\\\桌面\\\\uoft\\\\work_study\\\\ahkz_sentiment_classifier\\\\ahkz_sentiment_classifier\\\\ahkz_sentiment_classifier\\\\'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
    "sent_model = torch.load(ROOT_PATH + 'model\\\\3_class_fed_testimony_bert_large_uncased_best_model.pt', map_location = device)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ebdf1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_finbert = BertTokenizer.from_pretrained('ProsusAI/finbert')\n",
    "finbert_model = BertForSequenceClassification.from_pretrained('ProsusAI/finbert', num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54de9a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_file(files):\n",
    "    source_folder = \"C:\\\\Users\\\\czhao\\\\OneDrive\\\\桌面\\\\uoft\\\\work_study\\\\Chen_workingfolder\\\\Powell_Titles\\\\\"\n",
    "    target_folder = \"C:\\\\Users\\\\czhao\\\\OneDrive\\\\桌面\\\\uoft\\\\work_study\\\\Chen_workingfolder\\\\Powell_Titles_Result\\\\\"\n",
    "    \n",
    "    column_ls = ['sentence_before', 'sentence_with_the_quote', 'sentence_after', 'concat_sentence']\n",
    "#     column_ls = ['Title of article']\n",
    "    \n",
    "    for file in files:\n",
    "        try:\n",
    "            print(source_folder+file)\n",
    "            df = pd.read_csv(source_folder+file)\n",
    "            df['concat_sentence'] = df[['sentence_before', 'sentence_with_the_quote', 'sentence_after']].agg(' '.join, axis=1)\n",
    "        \n",
    "            for col in column_ls:\n",
    "                print(col)\n",
    "                # FinBERT\n",
    "                df['finbert_score'] = df[col].apply(lambda x: get_finbert_sentiment(x) if x!='' else ['','',''])\n",
    "\n",
    "                df[['finbert_neutral_'+col,'finbert_positive_'+col,'finbert_negative_'+col]] = \\\n",
    "                                pd.DataFrame(df['finbert_score'].tolist(), index= df.index)\n",
    "\n",
    "                df = df.drop(['finbert_score'], axis=1)\n",
    "\n",
    "                #ahkz\n",
    "                df['sentiment_value'] = 0\n",
    "\n",
    "                testimony_data_loader = create_data_loader(df, tokenizer, MAX_LEN, BATCH_SIZE, col)\n",
    "                testimony_texts, predictions, prediction_probs, real_values = get_predictions(sent_model, testimony_data_loader)\n",
    "\n",
    "                pred_numpy = predictions.numpy()\n",
    "                pred_probs = prediction_probs.numpy()\n",
    "                ms_pred = real_values.numpy()\n",
    "\n",
    "                df['sentiment_head'] = pred_numpy\n",
    "                df['sentiment_head'].replace({2: -1}, inplace=True)\n",
    "                df.drop('sentiment_value', axis=1, inplace=True)\n",
    "                df = df.rename(columns={\"sentiment_head\": \"ahkz_score_\"+col})\n",
    "\n",
    "                #pysent3\n",
    "                hiv4 = ps.HIV4()\n",
    "                df['pysent3'] = df[col].apply(lambda x: hiv4.get_score(hiv4.tokenize(x)))\n",
    "\n",
    "                pysent3_df = df['pysent3'].apply(pd.Series).rename(columns={\"Positive\": \"pysent3_Positive_\"+col, \\\n",
    "                                                               \"Negative\": \"pysent3_Negative_\"+col, \\\n",
    "                                                               \"Polarity\": \"pysent3_Polarity_\"+col, \\\n",
    "                                                               \"Subjectivity\": \"pysent3_Subjectivity_\"+col})\n",
    "\n",
    "                df = pd.concat([df, pysent3_df], axis=1)\n",
    "\n",
    "                df = df.drop(columns=['pysent3'])\n",
    "                \n",
    "                df['spacy_senti_'+col] = df[col].apply(lambda x: nlp_spacy(x).cats['positive'])\n",
    "                \n",
    "                df[['flesch_reading_ease_'+column, 'flesch_kincaid_grade_'+column, 'automated_readability_index_'+column, 'dale_chall_readability_score_'+column, 'gunning_fog_'+column, 'polysyllabcount_'+column]] = df.apply(lambda x: sentence_process(x[column]), axis=1, result_type=\"expand\")\n",
    "\n",
    "                \n",
    "            print('complete')\n",
    "\n",
    "            df.to_csv(target_folder + file, index =False)\n",
    "        except:\n",
    "            print(source_folder+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe6af3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\czhao\\OneDrive\\桌面\\uoft\\work_study\\Chen_workingfolder\\Powell_Titles\\factiva_10days_news_20180227_Powell_direct_quotes.csv\n",
      "sentence_before\n",
      "C:\\Users\\czhao\\OneDrive\\桌面\\uoft\\work_study\\Chen_workingfolder\\Powell_Titles\\factiva_10days_news_20180227_Powell_direct_quotes.csv\n",
      "C:\\Users\\czhao\\OneDrive\\桌面\\uoft\\work_study\\Chen_workingfolder\\Powell_Titles\\factiva_10days_news_20180717_Powell_direct_quotes.csv\n",
      "sentence_before\n",
      "C:\\Users\\czhao\\OneDrive\\桌面\\uoft\\work_study\\Chen_workingfolder\\Powell_Titles\\factiva_10days_news_20180717_Powell_direct_quotes.csv\n",
      "C:\\Users\\czhao\\OneDrive\\桌面\\uoft\\work_study\\Chen_workingfolder\\Powell_Titles\\factiva_10days_news_20190226_Powell_direct_quotes.csv\n",
      "sentence_before\n"
     ]
    }
   ],
   "source": [
    "processing_file(csv_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b81db2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tzip warning: zip file empty\r\n"
     ]
    }
   ],
   "source": [
    "!zip -r ../breakingnews.zip . -i ../target/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ab7e37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
